{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning: prediction\n",
    "\n",
    "Machine learning-trained algorithms can be used to predict the identity of sounds in recordings. For instance, an algorithm trained how to detect the sound of a Wood Thrush can be used to figure out where Wood Thrushes vocalize in a set of autonomous recordings. \n",
    "\n",
    "The Kitzes Lab, the developers of OpenSoundscape, pre-trained a series of [baseline machine learning models](https://pitt.box.com/s/a6jeamnew098vp5a9a7m1h9j5rce6t6y) that can be used to predict the presence of [506 species of common North American birds](https://pitt.app.box.com/s/d0snd1tyilscksbxc36q2slz6s4aa2ag). These are our \"beta\" models and are for demonstration purposes only, not for research use. We hope to make our more accurate models available soon. \n",
    "\n",
    "If you are interested in using these machine learning models for research, please contact us at the [Kitzes Lab](https://kitzeslab.org).\n",
    "\n",
    "This tutorial downloads an example model and demonstrates how to use it to predict the identity of birds in recordings. To download the tutorial as a Jupyter Notebook and run it on your own computer, click the \"Edit on GitHub\" button at the top right of the tutorial. You will have to [install OpenSoundscape](installation.html#installation) to use the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages\n",
    "Import the following modules to run a pre-trained machine learning learning classifier. First, from OpenSoundscape we will need two classes (`Audio` and `SingleTargetAudioDataset`) and three functions (`run_command`, `lowercase_annotations`, and `predict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.audio import Audio\n",
    "from opensoundscape.datasets import SingleTargetAudioDataset\n",
    "from opensoundscape.helpers import run_command\n",
    "from opensoundscape.raven import lowercase_annotations\n",
    "from opensoundscape.torch.predict import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following machine learning-related modules. OpenSoundscape uses PyTorch to do machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torchvision.models\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, use a few miscellaneous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model\n",
    "\n",
    "To use the model, it must be downloaded onto your computer and loaded with the same specifications it was created with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model\n",
    "Download the example model for Wood Thrush, *Hylocichla mustelina*. First, create a folder called `\"prediction_example\"` to store the model and its data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"prediction_example\"\n",
    "folder_path = Path(folder_name)\n",
    "if not folder_path.exists(): folder_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the model from the Box storage site using the following lines. If you prefer, you can also download the model off of [this](https://pitt.app.box.com/s/dslgslmag7y8ojqxv28mwhbnt7irpgeo) webpage. Make sure to move it into the `\"prediction_example\"` folder and ensure that it is named `\"hylocichla-mustelina-epoch-4.model\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_box(link, name):\n",
    "    run_command(f\"curl -L {link} -o ./{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = folder_path.joinpath(\"hylocichla-mustelina-epoch-4.model\")\n",
    "download_from_box(\n",
    "    link = \"https://pitt.box.com/shared/static/dslgslmag7y8ojqxv28mwhbnt7irpgeo.model\",\n",
    "    name = model_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "At its core, a machine learning model consists of two things: its architecture and its weights. \n",
    "\n",
    "The architecture is the complex structure of the model, which in this case, is a convolutional neural network. Convolutional neural networks are a particular set of algorithms especially suited to extracting and interpreting features from images, such as combinations of lines, dots, and edges. In this case, we use a `resnet18` convolutional neural network. After feature extraction, the convolutional neural network's features are passed to a classifier. The classifier decides how to weight each feature in predicting the final class identity. The model was trained with a `Linear` classifier.\n",
    "\n",
    "Create the architecture of the model. First, designate the model as a `resnet18` CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add the `fc` layers. \"FC\" stands for \"fully connected\". To set up the proper architecture, we need to specify the correct number of input features, output features, and classifier type. \n",
    "\n",
    "The number of input features to the FC is equal to the number of features extracted from the convolutional neural network and passed to the the FC layer: `model.fc.in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cnn_features = model.fc.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were trained to predict two classes (species present and species absent), so the number of output features of the FC layer is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the classifier type is a `torch.nn.Linear` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(\n",
    "    in_features = num_cnn_features,\n",
    "    out_features = num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the model are distinguished from its architecture because, while the architecture is decided by humans, the weights of the architecture are learned during the machine learning process. When downloading the machine learning model, you downloaded the weights. \n",
    "\n",
    "First, use `torch.load` to get the model weights from the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the weights into the architecture we have created. After this the model is almost ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prediction files\n",
    "\n",
    "To actually use the model, we need to download and prepare a set of recordings. The model was trained to make predictions on spectrograms made from 5 second-long recordings, so we will have to split the recordings up and transform them into spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "As example data, we have provided a 1 minute-long soundscape which contains Wood Thrush vocalizations. \n",
    "\n",
    "The following code downloads this audio file into the `\"prediction_example\"` folder created above. If you prefer, you can also download this file at [this link](https://pitt.box.com/shared/z73eked7quh1t2pp93axzrrpq6wwydx0). Make sure to move it into the `\"prediction_example\"` folder and ensure that it is named `\"1min.wav\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = folder_path.joinpath(\"1min.wav\")\n",
    "download_from_box(\n",
    "    link = \"https://pitt.box.com/shared/static/z73eked7quh1t2pp93axzrrpq6wwydx0.wav\",\n",
    "    name = data_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "The example soundscape must be split up into soundscapes of the same size as the ones the model was trained on. In this case, the soundscapes should be 5s long.\n",
    "\n",
    "First, create a directory in which to save split files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_directory = folder_path.joinpath(\"split_files\")\n",
    "if not split_directory.exists(): split_directory.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Audio(samples=(1920000,), sample_rate=32000)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_file = Audio.from_file(data_filename)\n",
    "base_file.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_split = [data_filename]\n",
    "split_directory = folder_path.joinpath(\"split_files\")\n",
    "dataset = SplitterDataset(\n",
    "    files_to_split,\n",
    "    overlap=0,\n",
    "    duration=5,\n",
    "    output_directory=split_directory,\n",
    "    include_last_segment=True\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=SplitterDataset.collate_fn,\n",
    ")\n",
    "\n",
    "results_csv = folder_path.joinpath(\"prediction_files.csv\")\n",
    "with open(results_csv, \"w\") as f:\n",
    "    if False:\n",
    "        f.write(\"Source,Annotations,Begin (s),End (s),Destination,Labels\\n\")\n",
    "    else:\n",
    "        f.write(\"Source,Begin (s),End (s),Destination\\n\")\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        for output in data:\n",
    "            f.write(f\"{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset\n",
    "\n",
    "Create a dataset from these data. We create a dictionary that associates numeric labels with the class names: 1 is for predicting a Wood Thrush's presence; 0 is for predicting a Wood Thrush's absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_analyze=list(split_directory.glob(\"*.wav\"))\n",
    "sample_df = pd.DataFrame(columns=['file'],data=files_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0:'hylocichla-mustelina-absent', 1:'hylocichla-mustelina-present'}\n",
    "test_dataset = SingleTargetAudioDataset(\n",
    "    sample_df,\n",
    "    filename_column = \"file\",\n",
    "    label_dict = label_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hylocichla-mustelina-absent</th>\n",
       "      <th>hylocichla-mustelina-present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/bc645003351149f4a7e2c7109b22afc1.wav</th>\n",
       "      <td>0.816133</td>\n",
       "      <td>-0.903320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/e36a0f200cdf42a23d49e78445121387.wav</th>\n",
       "      <td>1.480433</td>\n",
       "      <td>-0.927409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/4940c91a1837410240042cf55ccad568.wav</th>\n",
       "      <td>1.940377</td>\n",
       "      <td>-1.725088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/cfc05bd9e1b97eebdca3badc288de0cd.wav</th>\n",
       "      <td>2.629047</td>\n",
       "      <td>-1.988923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/32747f95e81ee34c56ed177c4f7e7df5.wav</th>\n",
       "      <td>2.513747</td>\n",
       "      <td>-2.366485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/369134205221b5a25fac0e264d0a1482.wav</th>\n",
       "      <td>2.351259</td>\n",
       "      <td>-1.628652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/f3d6aeabe7725f649dc56d6db04aa83f.wav</th>\n",
       "      <td>1.570931</td>\n",
       "      <td>-1.124706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/54534197c0768b6bb2a9305013e8c1af.wav</th>\n",
       "      <td>1.744635</td>\n",
       "      <td>-1.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/e0c2d4aed1d79d4a6194be948d3292da.wav</th>\n",
       "      <td>1.315882</td>\n",
       "      <td>-1.407135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/9d276a5dd54b631c4aa63da407a1225d.wav</th>\n",
       "      <td>1.766514</td>\n",
       "      <td>-1.096341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/636f23557581b700f286b7db29d01b61.wav</th>\n",
       "      <td>0.273381</td>\n",
       "      <td>-0.397208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_example/split_files/e55ba1b5a1316fcda5f3b4d73b2e36ee.wav</th>\n",
       "      <td>2.138355</td>\n",
       "      <td>-1.632506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hylocichla-mustelina-absent  \\\n",
       "prediction_example/split_files/bc645003351149f4...                     0.816133   \n",
       "prediction_example/split_files/e36a0f200cdf42a2...                     1.480433   \n",
       "prediction_example/split_files/4940c91a18374102...                     1.940377   \n",
       "prediction_example/split_files/cfc05bd9e1b97eeb...                     2.629047   \n",
       "prediction_example/split_files/32747f95e81ee34c...                     2.513747   \n",
       "prediction_example/split_files/369134205221b5a2...                     2.351259   \n",
       "prediction_example/split_files/f3d6aeabe7725f64...                     1.570931   \n",
       "prediction_example/split_files/54534197c0768b6b...                     1.744635   \n",
       "prediction_example/split_files/e0c2d4aed1d79d4a...                     1.315882   \n",
       "prediction_example/split_files/9d276a5dd54b631c...                     1.766514   \n",
       "prediction_example/split_files/636f23557581b700...                     0.273381   \n",
       "prediction_example/split_files/e55ba1b5a1316fcd...                     2.138355   \n",
       "\n",
       "                                                    hylocichla-mustelina-present  \n",
       "prediction_example/split_files/bc645003351149f4...                     -0.903320  \n",
       "prediction_example/split_files/e36a0f200cdf42a2...                     -0.927409  \n",
       "prediction_example/split_files/4940c91a18374102...                     -1.725088  \n",
       "prediction_example/split_files/cfc05bd9e1b97eeb...                     -1.988923  \n",
       "prediction_example/split_files/32747f95e81ee34c...                     -2.366485  \n",
       "prediction_example/split_files/369134205221b5a2...                     -1.628652  \n",
       "prediction_example/split_files/f3d6aeabe7725f64...                     -1.124706  \n",
       "prediction_example/split_files/54534197c0768b6b...                     -1.055664  \n",
       "prediction_example/split_files/e0c2d4aed1d79d4a...                     -1.407135  \n",
       "prediction_example/split_files/9d276a5dd54b631c...                     -1.096341  \n",
       "prediction_example/split_files/636f23557581b700...                     -0.397208  \n",
       "prediction_example/split_files/e55ba1b5a1316fcd...                     -1.632506  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "prediction_df = predict(model, test_dataset, label_dict=label_dict)\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command \"cleans up\" by deleting all the downloaded files and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSoundscape",
   "language": "python",
   "name": "opensoundscape-dxmth98s-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
