{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginner friendly training and prediction with CNNs\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a popular tool for developing automated machine learning classifiers on images or image-like samples. By converting audio into a two-dimensional frequency vs. time representation such as a spectrogram, we can generate image-like samples that can be used to train CNNs. This tutorial demonstrates the basic use of OpenSoundscape's `preprocessors` and `cnn` modules for training CNNs and making predictions using CNNs.\n",
    "\n",
    "Under the hood, OpenSoundscape uses Pytorch for machine learning tasks. By using OpenSoundscape's CNN classes such as `PytorchModel` in combination with preprocessor classes such as `CnnPreprocessor`, you can train and predict with PyTorch's powerful CNN architectures in just a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_dataloader' from 'opensoundscape.torch.models.utils' (/Users/SML161/opensoundscape/opensoundscape/torch/models/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/32pm1n6930l_kp8qmm0gfh100000gp/T/ipykernel_12934/3048712467.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preprocessor classes are used to load, transform, and augment audio samples for use in a machine learning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopensoundscape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpecPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the cnn module provides classes for training/predicting with various types of CNNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopensoundscape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opensoundscape/opensoundscape/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtaxa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opensoundscape/opensoundscape/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opensoundscape/opensoundscape/torch/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opensoundscape/opensoundscape/torch/models/cnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopensoundscape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcnn_architectures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from opensoundscape.torch.models.utils import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mBaseModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mget_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_dataloader' from 'opensoundscape.torch.models.utils' (/Users/SML161/opensoundscape/opensoundscape/torch/models/utils.py)"
     ]
    }
   ],
   "source": [
    "# Preprocessor classes are used to load, transform, and augment audio samples for use in a machine learning model\n",
    "from opensoundscape.preprocess.preprocessors import SpecPreprocessor\n",
    "\n",
    "# the cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape.torch.models.cnn import CNN\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import subprocess\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set manual seeds for pytorch and python. These ensure the training results are reproducible. You probably don't want to do this when you actually train your model, but it's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "### Download labeled audio files\n",
    "\n",
    "Training a machine learning model requires some pre-labeled data. These data, in the form of audio recordings or spectrograms, are labeled with whether or not they contain the sound of the species of interest. These data can be obtained from online databases such as Xeno-Canto.org, or by labeling one's own ARU data using a program like Cornell's Raven sound analysis software.\n",
    "\n",
    "The Kitzes Lab has created a small labeled dataset of short clips of American Woodcock vocalizations. You have two options for obtaining the folder of data, called `woodcock_labeled_data`:\n",
    "\n",
    "1. Run the following cell to download this small dataset. These commands require you to have `tar` installed on your computer, as they will download and unzip a compressed file in `.tar.gz` format. \n",
    "\n",
    "2. Download a `.zip` version of the files by clicking [here](https://pitt.box.com/shared/static/m0cmzebkr5qc49q9egxnrwwp50wi8zu5.zip). You will have to unzip this folder and place the unzipped folder in the same folder that this notebook is in.\n",
    "\n",
    "**Note**: Once you have the data, you do not need to run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['curl','https://pitt.box.com/shared/static/79fi7d715dulcldsy6uogz02rsn5uesd.gz','-L', '-o','woodcock_labeled_data.tar.gz']) # Download the data\n",
    "subprocess.run([\"tar\",\"-xzf\", \"woodcock_labeled_data.tar.gz\"]) # Unzip the downloaded tar.gz file\n",
    "subprocess.run([\"rm\", \"woodcock_labeled_data.tar.gz\"]) # Remove the file after its contents are unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one-hot encoded labels \n",
    "\n",
    "The folder contains 2s long audio clips taken from an autonomous recording unit. It also contains a file `woodcock_labels.csv` which contains the names of each file and its corresponding label information, created using a program called [Specky](https://github.com/rhine3/specky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Specky output: a table of labeled audio files\n",
    "specky_table = pd.read_csv(Path(\"woodcock_labeled_data/woodcock_labels.csv\"))\n",
    "specky_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table must provide an accurate path to the files of interest. For this self-contained tutorial, we can use relative paths (starting with a dot and referring to files in the same folder), but you may want to use absolute paths for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the paths to the audio files\n",
    "specky_table.filename = ['./woodcock_labeled_data/'+f for f in specky_table.filename]\n",
    "specky_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `categorical_to_one_hot` function from `opensoundscape.annotations` to crate \"one hot\" labels - that is, a column for every class, with 1 for present or 0 for absent in each sample's row. In this case, our classes are simply `'negative'` for files without a woodcock and `'positive'` for files with a woodcock. \n",
    "\n",
    "We'll need to put the paths to audio files as the index of the DataFrame.\n",
    "\n",
    "Note that these classes are mutually exclusive, so we have a \"single-target\" problem, as opposed to a \"multi-target\" problem where multiple classes can simultaneously be present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.annotations import categorical_to_one_hot\n",
    "one_hot_labels, classes = categorical_to_one_hot(specky_table[['woodcock']].values)\n",
    "labels = pd.DataFrame(index=specky_table['filename'],data=one_hot_labels,columns=classes)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to, we can always convert one_hot labels back to categorical labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.annotations import one_hot_to_categorical\n",
    "categorical_labels = one_hot_to_categorical(one_hot_labels,classes)\n",
    "categorical_labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation sets\n",
    "We use a utility from `sklearn` to randomly divide the labeled samples into two sets. The first set, `train_df`, will be used to train the CNN, while the second set, `valid_df`, will be used to test how well the model can predict the classes of samples that it was not trained with.\n",
    "\n",
    "During the training process, the CNN will go through all of the samples once every \"epoch\" for several (sometimes hundreds of) epochs. Each epoch usually consists of a \"learning\" step and a \"validation\" step. In the learning step, the CNN iterates through all of the training samples while the computer program is modifying the weights of the convolutional neural network. In the validation step, the program performs prediction on all of the validation samples and prints out metrics to assess how well the classifier generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,validation_df = train_test_split(labels,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train a model\n",
    "Now, we create a convolutional neural network model object, train it on the `train_dataset` with validation from `validation_dataset`\n",
    "\n",
    "### Set up a two-class, single-target model\n",
    "This demonstrates using a two class, single-target model. \n",
    "\n",
    "* The two classes in this case are \"positive\" and \"negative\" \n",
    "* The model is \"single target\", meaning that each sample belongs to exactly one class, \"positive\" or \"negative\" \n",
    "\n",
    "We usually use two-class, single-target models to predict the presence or absence of a single species. We often refer to this as a \"binary\" model, but be careful not to confuse this for thresholded \"binary\" output predictions (1 or 0).\n",
    "\n",
    "The model object should be initialized with a list of class names that matches the class names in the training dataset. Here we'll use the resnet18 architecture, a popular and powerful architecture that makes a good starting point. For more details on other CNN architectures, see the \"Advanced CNN Training\" tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "classes = train_df.columns\n",
    "model = CNN('resnet18',classes=classes,sample_duration=2.0,single_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect training images\n",
    "Before creating a machine learning algorithm, we strongly recommend making sure the images coming out of the preprocessor look like you expect them to. Here we generate images for a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess.utils import show_tensor #helper function to visualize processed samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load a handful of random samples, printing the labels and image for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick some random samples and make a dataset from the model's preprocessor\n",
    "#the dataset will generate samples using the combination of (1) a list of samples and (2) the preprocessor\n",
    "sample_of_3 = train_df.sample(n=3)\n",
    "sample_dataset = model.preprocessor.make_dataset(sample_of_3)\n",
    "\n",
    "for sample in sample_dataset:\n",
    "    print(f\"labels: {sample['y']}\")\n",
    "    #use invert=True and transform_from_zero_centered=True to get a normal-looking spectrogram\n",
    "    show_tensor(sample['X'],invert=True,transform_from_zero_centered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessor allows you to turn all augmentation off or on as desired. Inspect the unaugmented images as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset.augmentation_on=False\n",
    "for sample in sample_dataset:\n",
    "    print(f\"labels: {sample['y']}\")\n",
    "    show_tensor(sample['X'],invert=True,transform_from_zero_centered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Depending on the speed of your computer, training the CNN may take a few minutes.\n",
    "\n",
    "We'll only train for 5 epochs on this small dataset as a demonstration, but you'll probably need to train for tens (or hundreds) of epochs on hundreds (or thousands) of training files to create a useful model. \n",
    "\n",
    "Batch size refers to the number of samples that are simultaneously processed by the model. In practice, using larger batch sizes (64+) improves stability and generalizability of training, particularly for architectures (such as ResNet) that contain a 'batch norm' layer. Here we use a small batch size to keep the computational reqirements for this tutorial low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_df=train_df,\n",
    "    validation_df=validation_df,\n",
    "    save_path='./binary_train/', #where to save the trained model\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    save_interval=5, #save model every 5 epochs (the best model is always saved in addition)\n",
    "    num_workers=0, #specify 4 if you have 4 CPU processes, eg; 0 means only the root process\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss history\n",
    "We can plot the loss from each epoch to check that our loss is declining. Loss should decline as the model learns, but may have ups and downs along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.loss_hist.keys(),model.loss_hist.values())\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing and Logging outputs\n",
    "We can log the outputs of the training process to a file, and/or print them. We can independently modify how much content is logged/printed with the model's attributes `model.verbose` and `model.logging_level`. Content increases from level 0 (nothing) to 1 (standard), 2, 3, etc. For instance, let's train for an epoch with lots of logged content but no printed output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.logging_level = 3 #request lots of logged content\n",
    "model.log_file = './binary_train/training_log.txt' #specify a file to log output to\n",
    "Path(model.log_file).parent.mkdir(parents=True,exist_ok=True) #make the folder ./binary_train\n",
    "\n",
    "model.verbose = 0 #don't print anything to the screen during training\n",
    "model.train(\n",
    "    train_df=train_df,\n",
    "    validation_df=validation_df,\n",
    "    save_path='./binary_train/', #where to save the trained model\n",
    "    epochs=1,\n",
    "    batch_size=8,\n",
    "    save_interval=5, #save model every 5 epochs (the best model is always saved in addition)\n",
    "    num_workers=0, #specify 4 if you have 4 CPU processes, eg; 0 means only the root process\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "We haven't actually trained a useful model in 5 epochs, but we can use the trained model to demonstrate how prediction works and show several of the settings useful for prediction.\n",
    "\n",
    "We will run prediction on two one-minute clips of field data recorded by an AudioMoth acoustic recorded. The two files are located in `woodcock_labeled_data/field_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the field data\n",
    "\n",
    "To run prediction, also known as \"inference\", wich a CNN, we simply call model's `predict` method and pass it a list of file paths (or a dataframe with file paths in the index). \n",
    "\n",
    "The predict function will internally split audio files into the appropriate length clips for prediction and generate prediction scores for each clip. \n",
    "\n",
    "- By default, there is no overlap between these clips, but we can specify a fraction of overlap with consecutive clips with the `overlap_fraction` argument (eg, 0.5 for 50% overlap). \n",
    "\n",
    "- Additionally, if we want to predict on audio files that are already trimmed to the same duration as the training files, we can specify `split_files_into_clips=False`. \n",
    "\n",
    "Calling `.predict()` will return three things:\n",
    "\n",
    "- scores dataframe: numeric predictions from the model for each sample and class (by default these are raw outputs from the model)\n",
    "\n",
    "- predictions dataframe: 0/1 predictions from the model for each sample and class (only generated if `binary_predictions` argument is supplied)\n",
    "\n",
    "- unsafe_samples: list of any samples that failed to load properly\n",
    "\n",
    "Let's predict on the two field recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "field_recordings = glob('./woodcock_labeled_data/field_data/*')\n",
    "field_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores_df, prediction_binary_df, unsafe_samples = model.predict(field_recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function generated a dataframe with rows for each 2-second segment of each 1-minute audio clip. Let's look at the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary 0/1 predictions were not generated because the `binary_predictions` argument was not supplied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prediction_binary_df` dataframe is `None` - this is because we haven't specified an option for the `binary_preds` argument of predict. We can choose between `'single_target'` prediction (always predict the highest scoring class and no others) or `'multi_target'` (predict 1 for all classes exceeding a threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_binary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of the samples were processed without errors, the unsafe_samples list will be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create presence/absence (0/1) predictions\n",
    "\n",
    "Supplying the `binary_preds` argument returns a dataframe in which the scores are transformed from continuous numbers to either 0 or 1. \n",
    "\n",
    "**Note**: Binary predictions always have some error rates, sometimes large ones. It is not generally advisable to use these binary predictions as scientific observations without a thorough understanding of the model's false-positive and false-negative rates.\n",
    "\n",
    "If you wish to output binary predictions, three options are available:\n",
    "\n",
    "* `None`: default. do not create or return binary predictions\n",
    "* `'single_target'`: predict that the highest-scoring class = 1, all others = 0\n",
    "* `'multi_target'`: provide a `threshold`. Scores above threshold = 1, others = 0\n",
    "\n",
    "For instance, using the option `'single_target'` chooses whichever of `'negative'` or `'positive'` is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,preds,labels = model.predict(field_recordings,binary_preds='single_target')\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'multi_target'` option allows you to select a threshold. If a score meets that threshold, the binary prediction is 1; otherwise, it is 0. \n",
    "\n",
    "Each score will have a function applied to it that takes the score from the real numbers, (-inf, inf), to the range [0, 1] (specifically the logistic sigmoid, or [expit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html) function). Whether the score meets this threshold will be based off of the sigmoid, not the raw score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df, pred_df, label_df = model.predict(\n",
    "    validation_df,\n",
    "    binary_preds='multi_target',\n",
    "    threshold=0.99,\n",
    ")\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in some of the above predictions, both the negative and positive classes are predicted to be present. This is because the `'multi_target'` option assumes that the classes are not mutually exclusive. For a presence/absence model like the one above, the `'single_target'` option is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the activation layer\n",
    "We can modify the final activation layer to change the scores returned by the `predict()` function. Note that this does not impact the results of the binary predictions (described above), which are always calculated using a sigmoid transformation (for multi-target models) or softmax function (for single-target models).\n",
    "\n",
    "Options include:\n",
    "\n",
    "* `None`: default. Just the raw outputs of the network, which are in (-inf, inf)\n",
    "* `'softmax'`: scores across all classes will sum to 1 for each sample\n",
    "* `'softmax_and_logit'`: softmax the scores across all classes so they sum to 1, then apply the \"logit\" transformation to these scores, taking them from [0,1] back to (-inf,inf)\n",
    "* `'sigmoid'`: transforms each score individually to [0, 1] without requiring they sum to 1\n",
    "\n",
    "In this case, since we are choosing between two mutually exclusive classes, we want to use the `'softmax'` activation.\n",
    "\n",
    "Let's generate binary 0/1 predictions on the validation set. Since these samples are the same length as the training files, we'll specify `split_files_into_clips=False` (we just want one prediction per file, we don't want to divide each file into shorter clips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores, valid_preds, unsafe_samples = model.predict(\n",
    "    validation_df, \n",
    "    activation_layer='softmax',\n",
    "    split_files_into_clips=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the softmax scores to the true labels for this dataset, side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores.columns = ['pred_negative','pred_positive']\n",
    "validation_df.join(valid_scores).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that our model hasn't really learned anything yet. It just predicts everything is a negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parameters can be used to increase prediction efficiency, depending on the computational resources available:\n",
    "\n",
    "- `num_workers`: Pytorch's method of parallelizing across cores (CPUs) - choose 0 to predict on the root process, or >1 if you want to use more than 1 CPU process. \n",
    "- `batch_size`: number of samples to predict on simultaneously. You can try increasing this by factors of two until you get a memory error, which means your batch size is too large for your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df, pred_df, label_df = model.predict(\n",
    "    validation_df,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    binary_preds='multi_target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class models\n",
    "A multi-class model can have any number of classes, and can be either \n",
    "\n",
    "- multi-target: any number of classes can be positive for one sample\n",
    "- single-target: exactly one class is positive for each sample\n",
    "\n",
    "Models that are multi-target benefit from a modified loss function, and we have implemented a special class that is specifically designed for multi-target problems called `ResampleLoss`. We can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import use_resample_loss\n",
    "model = CNN('resnet18',classes,2.0,single_target=False)\n",
    "use_resample_loss(model)\n",
    "print(\"model.single_target:\", model.single_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Training looks the same as in two-class models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_df,\n",
    "    validation_df,\n",
    "    save_path='./multilabel_train/',\n",
    "    epochs=1,\n",
    "    batch_size=16,\n",
    "    save_interval=100,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "Prediction looks the same as demonstrated above, but make sure to think carefully:\n",
    "\n",
    "* What `activation_layer` do you want? \n",
    "* If outputting binary predictions for each sample and class, is my model single-target (`binary_preds='single_target'`) or multi-target (`binary_preds='multi_target'`)?\n",
    "\n",
    "For more detail on these choices, see the sections about activation layers and binary predictions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds,_,_ = model.predict(train_df,split_files_into_clips=False,)\n",
    "train_preds.columns = ['pred_negative','pred_positive']\n",
    "train_df.join(train_preds).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load models\n",
    "\n",
    "Models can be easily saved to a file and loaded at a later time. If the model was saved with OpenSoundscape version >=0.6.1, the entire model object will be saved - including the class, cnn architecture, loss function, and training/validation datasets. Models saved with earlier versions of OpenSoundscape do not contain all of this information and may require that you know their class and architecture (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load a model \n",
    "\n",
    "OpenSoundscape saves models automatically during training:\n",
    "\n",
    "* The model saves weights to `self.save_path` to `epoch-X.model` automatically during training every `save_interval` epochs\n",
    "* The model keeps the file `best.model` updated with the weights that achieve the best score on the validation dataset. By default the model is evaluated using the mean average precision (MAP) score, but you can overwrite `model.eval()` if you want to use a different metric for the best model. \n",
    "\n",
    "You can also save the model manually at any time with `model.save(path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CNN('resnet18',classes,2.0,single_target=True)\n",
    "# Save every 2 epochs\n",
    "model1.train(\n",
    "    train_df,\n",
    "    validation_df,\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    save_path='./binary_train/',\n",
    "    save_interval=2,\n",
    "    num_workers=0\n",
    ")\n",
    "model1.save('./binary_train/my_favorite.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on saving models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that loading a model in a different version of OpenSoundscape than the version that saved the model may not work. To use a model across different versions of OpenSoundscape, you should save the model.network's state dict as described in the \"predicting with pre-trained models\" tutorial. OpenSoundscape includes helper functions .save_weights() and .load_weights() which allow you to save and load the model.network's state dict easily. We recommend saving both the full model object (`.save()`) and the raw weights (`.save_weights()`) for models you plan to use in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-load a saved model with the load_model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import load_model\n",
    "model = load_model('./binary_train/best.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be used for prediction (`model.predict()`) or to continue training (`model.train()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using saved (or pre-trained) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a saved or downloaded model to run predictions on audio files is as simple as\n",
    "\n",
    "1. Loading a previously saved model\n",
    "2. Creating an instance of a preprocessor class for prediction\n",
    "3. Running `model.predict()` on the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "model = load_model('./binary_train/best.model')\n",
    "\n",
    "#predict on a dataset\n",
    "scores,_,_ = model.predict(field_recordings, activation_layer='softmax_and_logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: see the tutorial \"predicting with pre-trained models\" for loading and using models from earlier OpenSoundscape versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training from saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to predicting using a saved model, we can also continue to train a model after loading it from a saved file. \n",
    "\n",
    "By default, `.load()` loads the optimizer parameters and learning rate parameters from the saved model, in addition to the network weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create architecture\n",
    "model = load_model('./binary_train/best.model')\n",
    "\n",
    "# Continue training from the checkpoint where the model was saved\n",
    "model.train(train_df,validation_df,save_path='.',epochs=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have seen the basic usage of training CNNs with OpenSoundscape and generating predictions.  \n",
    "\n",
    "Additional tutorials you might be interested in are:\n",
    "* [Custom preprocessing](preprocessors.html): how to change spectrogram parameters, modify augmentation routines, etc.\n",
    "* [Custom training](cnn_training_advanced.html): how to modify and customize model training\n",
    "* [Predict with pre-trained CNNs](predict_with_pretrained_cnn.html): details on how to predict with pre-trained CNNs. Much of this information was covered in the tutorial above, but this tutorial also includes information about using models made with previous versions of OpenSoundscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, clean up and remove files created during this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "dirs = ['./multilabel_train', './binary_train', './woodcock_labeled_data']\n",
    "[shutil.rmtree(d) for d in dirs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_dev",
   "language": "python",
   "name": "opso_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
