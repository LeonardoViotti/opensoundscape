{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6e344-3ed7-4b0a-aa36-2b83d4842bff",
   "metadata": {},
   "source": [
    "# Train a CNN\n",
    "\n",
    "Convolutional neural networks (CNNs) are popular tools for developing automated machine learning classifiers on images or image-like samples. By converting audio into a two-dimensional frequency vs. time representation such as a spectrogram, we can generate image-like samples that can be used to train CNNs. \n",
    "\n",
    "This tutorial demonstrates the basic use of OpenSoundscape's `preprocessors` and `cnn` modules for training CNNs and making predictions using CNNs.\n",
    "\n",
    "Under the hood, OpenSoundscape uses Pytorch for machine learning tasks. By using the class `opensoundscape.ml.cnn.CNN`, you can train and predict with PyTorch's powerful CNN architectures in just a few lines of code. \n",
    "\n",
    "## Run this tutorial\n",
    "\n",
    "This tutorial is more than a reference! It's a Jupyter Notebook which you can run and modify on Google Colab or your own computer.\n",
    "\n",
    "|Link to tutorial|How to run tutorial|\n",
    "| :- | :- |\n",
    "| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kitzeslab/opensoundscape/blob/master/docs/tutorials/train_cnn.ipynb) | The link opens the tutorial in Google Colab. Uncomment the \"installation\" line in the first cell to install OpenSoundscape. |\n",
    "| [![Download via DownGit](https://img.shields.io/badge/GitHub-Download-teal?logo=github)](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/kitzeslab/opensoundscape/blob/master/docs/tutorials/train_cnn.ipynb) | The link downloads the tutorial file to your computer. Follow the [Jupyter installation instructions](https://opensoundscape.org/en/latest/installation/jupyter.html), then open the tutorial file in Jupyter. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ecca1-702b-4fa3-a48b-61025f55d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to download OpenSoundscape for use in Google Colab\n",
    "#!pip install opensoundscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d88b73-77d1-4c00-a83a-8466fd79e15e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9eee8-c65c-4df1-95d0-15dda341ee0a",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "972e3e01-c85f-415d-95cc-9b695332f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape import CNN\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import subprocess\n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adf5d6-403d-4a06-bc85-477cdc60ec07",
   "metadata": {},
   "source": [
    "### Set random seeds\n",
    "\n",
    "Set manual seeds for Pytorch and Python. These ensure the training results are reproducible. You probably don't want to do this when you actually train your model, but it's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e09bd5-e86d-44e0-8ffa-0f8ee699c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c60bac-280a-4d72-80b6-2659f6ecd83d",
   "metadata": {},
   "source": [
    "### Download files\n",
    "\n",
    "Training a machine learning model requires some pre-labeled data. These data, in the form of audio recordings or spectrograms, are labeled with whether or not they contain the sound of the species of interest. \n",
    "\n",
    "These data can be obtained from online databases such as Xeno-Canto.org, or by labeling one's own ARU data using a program like Cornell's Raven sound analysis software. In this example we are using a set of annotated avian soundscape recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<blockquote><i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i> Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes.\n",
    "https://doi.org/10.1002/ecy.3329 \n",
    "</blockquote>\n",
    "\n",
    "These are the same data that are used by the annotation and preprocessing tutorials, so you can skip this step if you've already downloaded them there.\n",
    "\n",
    "Download the datasets to your current working directory and unzip them. You can do so by running the cell below OR\n",
    "- downloading and unzipping both `annotation_Files.zip` and `mp3_Files.zip` from the https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z  \n",
    "- Move the unziped contents of each into the folder `./resources/02/annotated_data/` (a subfolder of the current folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8bf5cf-6c0b-43d6-a3bc-62657597fbec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-06 12:34:19--  https://datadryad.org/stash/downloads/file_stream/641805\n",
      "Resolving datadryad.org (datadryad.org)... 54.187.107.134, 54.185.232.144\n",
      "Connecting to datadryad.org (datadryad.org)|54.187.107.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dryad-assetstore-merritt-west.s3.us-west-2.amazonaws.com/ark%3A/13030/m5799nzg%7C1%7Cproducer/annotation_Files.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2KERHV5E3OITXZXC%2F20231006%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231006T163420Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=4b4e8b760e41d2e3695671efea763389a4b7955d0c2a8e6ec5220b9348f989e3 [following]\n",
      "--2023-10-06 12:34:19--  https://dryad-assetstore-merritt-west.s3.us-west-2.amazonaws.com/ark%3A/13030/m5799nzg%7C1%7Cproducer/annotation_Files.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2KERHV5E3OITXZXC%2F20231006%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231006T163420Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=4b4e8b760e41d2e3695671efea763389a4b7955d0c2a8e6ec5220b9348f989e3\n",
      "Resolving dryad-assetstore-merritt-west.s3.us-west-2.amazonaws.com (dryad-assetstore-merritt-west.s3.us-west-2.amazonaws.com)... ^C\n",
      "--2023-10-06 12:34:20--  https://datadryad.org/stash/downloads/file_stream/641807\n",
      "Resolving datadryad.org (datadryad.org)... 54.185.232.144, 54.187.107.134\n",
      "Connecting to datadryad.org (datadryad.org)|54.185.232.144|:443... ^C\n",
      "Archive:  annotation_Files.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of annotation_Files.zip or\n",
      "        annotation_Files.zip.zip, and cannot find annotation_Files.zip.ZIP, period.\n",
      "Archive:  mp3_Files.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of mp3_Files.zip or\n",
      "        mp3_Files.zip.zip, and cannot find mp3_Files.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# Note: the \"!\" preceding each line below allows us to run bash commands in a Jupyter notebook\n",
    "# If you are not running this code in a notebook, input these commands into your terminal instead\n",
    "!wget -O annotation_Files.zip https://datadryad.org/stash/downloads/file_stream/641805\n",
    "!wget -O mp3_Files.zip https://datadryad.org/stash/downloads/file_stream/641807\n",
    "!unzip annotation_Files.zip -d ./resources/02/annotated_data/Annotation_Files\n",
    "!unzip mp3_Files.zip -d ./resources/02/annotated_data/Recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82705d0a-f5f7-4104-8ea7-461ca7f72e4e",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "To prepare audio data for machine learning, we need to convert our annotated data into clip-level labels.\n",
    "\n",
    "These steps are covered in depth in other tutorials, so we'll just set our clip labels up quickly for this example.\n",
    "\n",
    "First, get exactly matched lists of audio files and their corresponding selection files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cbd28e-1e20-4709-95e7-dadf7f8b3f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./annotated_data/\")\n",
    "\n",
    "# Make a list of all of the selection table files\n",
    "selection_files = glob(f\"{dataset_path}/Annotation_Files/*/*.txt\")\n",
    "\n",
    "# Create a list of audio files, one corresponding to each Raven file\n",
    "# (Audio files have the same names as selection files with a different extension)\n",
    "audio_files = [f.replace('Annotation_Files','Recordings').replace('.Table.1.selections.txt','.mp3') for f in selection_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6709e-9508-4f08-b1ea-30d8662161b1",
   "metadata": {},
   "source": [
    "Next, convert the selection files and audio files to a `BoxedAnnotations` object, which contains the time, frequency, and label information for all annotations for every recording in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f3f7a5-e074-4313-a1bd-6b5a4c98612e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tessa/Code/opensoundscape/opensoundscape/annotations.py:215: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_annotations = pd.concat(all_file_dfs).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.annotations import BoxedAnnotations\n",
    "# Create a dataframe of annotations\n",
    "annotations = BoxedAnnotations.from_raven_files(\n",
    "    selection_files,\n",
    "    audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8c74cb-3fbf-4f29-8ed5-d62f51b645a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters to use for label creation\n",
    "clip_duration = 3\n",
    "clip_overlap = 0\n",
    "min_label_overlap = 0.25\n",
    "species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "\n",
    "# Create dataframe of one-hot labels\n",
    "clip_labels = annotations.one_hot_clip_labels(\n",
    "    clip_duration = clip_duration, \n",
    "    clip_overlap = clip_overlap,\n",
    "    min_label_overlap = min_label_overlap,\n",
    "    class_subset = species_of_interest # You can comment this line out if you want to include all species.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d2b3ae-a37b-4e2a-a0c0-4bd41fce40ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NOCA</th>\n",
       "      <th>EATO</th>\n",
       "      <th>SCTA</th>\n",
       "      <th>BAWW</th>\n",
       "      <th>BCCH</th>\n",
       "      <th>AMCR</th>\n",
       "      <th>NOFL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">annotated_data/Recordings/Recording_1/Recording_1_Segment_07.mp3</th>\n",
       "      <th>0.0</th>\n",
       "      <th>3.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <th>6.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <th>9.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        NOCA  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        1.0   \n",
       "                                                   3.0        6.0        1.0   \n",
       "                                                   6.0        9.0        1.0   \n",
       "                                                   9.0        12.0       1.0   \n",
       "                                                   12.0       15.0       1.0   \n",
       "\n",
       "                                                                        EATO  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0   \n",
       "                                                   3.0        6.0        1.0   \n",
       "                                                   6.0        9.0        1.0   \n",
       "                                                   9.0        12.0       1.0   \n",
       "                                                   12.0       15.0       1.0   \n",
       "\n",
       "                                                                        SCTA  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0   \n",
       "                                                   3.0        6.0        0.0   \n",
       "                                                   6.0        9.0        0.0   \n",
       "                                                   9.0        12.0       0.0   \n",
       "                                                   12.0       15.0       0.0   \n",
       "\n",
       "                                                                        BAWW  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0   \n",
       "                                                   3.0        6.0        0.0   \n",
       "                                                   6.0        9.0        0.0   \n",
       "                                                   9.0        12.0       0.0   \n",
       "                                                   12.0       15.0       0.0   \n",
       "\n",
       "                                                                        BCCH  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0   \n",
       "                                                   3.0        6.0        1.0   \n",
       "                                                   6.0        9.0        1.0   \n",
       "                                                   9.0        12.0       1.0   \n",
       "                                                   12.0       15.0       0.0   \n",
       "\n",
       "                                                                        AMCR  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0   \n",
       "                                                   3.0        6.0        0.0   \n",
       "                                                   6.0        9.0        0.0   \n",
       "                                                   9.0        12.0       0.0   \n",
       "                                                   12.0       15.0       0.0   \n",
       "\n",
       "                                                                        NOFL  \n",
       "file                                               start_time end_time        \n",
       "annotated_data/Recordings/Recording_1/Recording... 0.0        3.0        0.0  \n",
       "                                                   3.0        6.0        0.0  \n",
       "                                                   6.0        9.0        0.0  \n",
       "                                                   9.0        12.0       0.0  \n",
       "                                                   12.0       15.0       0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec6fac-fb79-43dc-86c9-d66230189a94",
   "metadata": {},
   "source": [
    "## Create train, validation, and test datasets\n",
    "\n",
    "To train and test a model, we use three datasets:\n",
    "* The **training dataset** is used to fit your machine learning model to the audio data. \n",
    "* The **validation dataset** is a held-out dataset that is used to select hyperparameters (e.g. how many epochs to train for) during training\n",
    "* The **test dataset** is another held-out dataset that we use to check how the model performs on data that were not available at all during training. \n",
    "\n",
    "While both the training and validation datasets are used while trained the model, the test dataset is never touched until the model is fully trained and completed.\n",
    "\n",
    "The training and validation datasets may be gathered from the same source as each other. In contrast, the test dataset is often gathered from a different source to assess whether the model's performance generalizes to a real-world problem. For example, training and validation data might be drawn from an online database like Xeno-Canto, whereas the testing data is from your own field data. \n",
    "\n",
    "### Create a test dataset\n",
    "\n",
    "We'll separate the test dataset first. For a good assessment of the model's generalization, we want the test set to be independent of the training and validation datasets. For example, we don't want to use clips from the same source recording in the training dataset and the test dataset.\n",
    "\n",
    "For this example, we'll use the recordings in the folders `Recording_1`, `Recording_2` and `Recording_3` as our training and validation data, and use the recordings in folder `Recording_4` as our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8190cbf-d9ad-400d-ad44-789eead2a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all files from Recording_4 as a test set\n",
    "mask = clip_labels.reset_index()['file'].apply(lambda x: 'Recording_4' in x).values\n",
    "test_set = clip_labels[mask]\n",
    "\n",
    "# All other files will be used as a training set\n",
    "train_and_val_set = clip_labels.drop(test_set.index)\n",
    "\n",
    "# Save .csv tables of the training and validation sets to keep a record of them\n",
    "train_and_val_set.to_csv(\"./annotated_data/train_and_val_set.csv\")\n",
    "test_set.to_csv(\"./annotated_data/test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fe29b-d6e7-4593-9b44-813c5aafb00b",
   "metadata": {},
   "source": [
    "If you wanted, you could load the training and testing set from these saved CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f53802-c25f-4cbe-ab7f-531b80f38cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_set = pd.read_csv('./annotated_data/training_set.csv',index_col=[0,1,2])\n",
    "test_set = pd.read_csv('./annotated_data/test_set.csv',index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb99584-33fc-4889-83b5-4c912e3c3188",
   "metadata": {},
   "source": [
    "### Split training and validation datasets\n",
    "\n",
    "Now, separate the remaining non-test data into training and validation datasets.\n",
    "\n",
    "The idea of keeping a separate validation dataset is that, throughout training, we can 'peek' at the performance on the validation set to choose hyperparameters. (This is in contrast to the test dataset, which we will not look at until we've finished training our model.)\n",
    "\n",
    "One important hyperparameter is the number of **epochs** to train to, in order to prevent overfitting. Each epoch includes one round of fitting on each training sample. \n",
    "\n",
    "If a model's performance on a training dataset continues to improve as it trains, but its performance on the validation dataset plateaus, this could incate the model is **overfitting** on the training dataset, learning information specific to those particular samples instead of gaining the ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f47db9c-bf65-46b9-b64b-040d13ea17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training data into training and validation sets\n",
    "train_df, valid_df = sklearn.model_selection.train_test_split(train_and_val_set, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d30e3e-eda1-4476-8ebf-db4b0844a1d0",
   "metadata": {},
   "source": [
    "### Resample data for even class representation\n",
    "\n",
    "Before training, we will balance the number of samples of each class in the training set. This helps the model learn all of the classes, rather than paying too much attention to the classes with the most labeled annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a75f8ae-c81b-4a1b-b62e-87fe1b64eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.data_selection import resample\n",
    "\n",
    "# upsample (repeat samples) so that all classes have 800 samples\n",
    "balanced_train_df = resample(train_df,n_samples_per_class=800,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9730295-df2d-4fca-85d8-a7d756b1763f",
   "metadata": {},
   "source": [
    "## Set up model\n",
    "\n",
    "Now we create a model object. We have to select several parameters when creating this object: its `architecture`, `classes`, and `sample_duration`. \n",
    "\n",
    "Some additional parameters can also be changed at this step, which we cover in a later tutorial (\"Customize CNN training\"). \n",
    "\n",
    "One optional parameter is quite important (e.g. whether the model should be trained to detect one or multiple classes per clip, `single_target=True` or `single_target=False`, so we'll set that manually here as well.\n",
    "\n",
    "### Choose architecture\n",
    "\n",
    "The `architecture` is the particular design of the CNN. This option can either be a string matching one of the architectures included in OpenSoundscape, or a custom PyTorch model object.\n",
    "\n",
    "See what string architectures are available in OpenSoundscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41681ed-3a6d-4cb6-ac2e-9bda2b73edfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'alexnet',\n",
       " 'vgg11_bn',\n",
       " 'squeezenet1_0',\n",
       " 'densenet121',\n",
       " 'inception_v3',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_widese_b0',\n",
       " 'efficientnet_widese_b4']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opensoundscape.ml\n",
    "opensoundscape.ml.cnn_architectures.list_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5961e37-8441-493f-a626-7affcc68e563",
   "metadata": {},
   "source": [
    "We will choose `resnet34` for this example. For more information on choosing architectures, see the \"Customize CNN training\" tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd5d6ffe-636e-4033-b503-4921cb9136a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'resnet34'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66d592-fb5b-4e9d-a832-9ae123b9a442",
   "metadata": {},
   "source": [
    "### Create CNN object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5061ad-3fae-4b00-967e-f1101ff5165e",
   "metadata": {},
   "source": [
    "Now, create a CNN object with this architecture, the classes we put into the dataframe above, and the same sample duration as we selected above.\n",
    "\n",
    "The first time you run this script for a particular architecture, OpenSoundscape will download the desired architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c61f98fb-0791-4e3d-ab51-ee36ae3e1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN object designed to recognize 3-second samples\n",
    "from opensoundscape import CNN\n",
    "\n",
    "# Can use this code to get your classes, if needed\n",
    "class_list = list(train_df.columns)\n",
    "\n",
    "model = CNN(\n",
    "    architecture = architecture,\n",
    "    classes = class_list,\n",
    "    sample_duration = clip_duration #3s, selected above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a8de1-3d6b-4f03-bd61-dae8c17f1ddf",
   "metadata": {},
   "source": [
    "### Set up GPU\n",
    "\n",
    "If a GPU is available on your computer, these steps will move the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9de0c6df-d999-4791-b358-312a076f6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device is: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    model.device='mps' #Apple Silicon\n",
    "elif torch.cuda.is_available():\n",
    "    model.device='cuda' #CUDA GPU  \n",
    "print(f'model.device is: {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c901111-323f-485d-bb45-f97a8abedafb",
   "metadata": {},
   "source": [
    "### Set up model logging\n",
    "\n",
    "While this step is optional, it is very helpful for model training. In this step, we set up model logging on a service called **Weights & Biases** (AKA `wandb`). \n",
    "\n",
    "Weights & Biases is a free website you can use to monitor model training. It is integrated with OpenSoundscape to include helpful functions such as checking on your model's training progress in real time, visualizing the spectrograms created for training your model, comparing multiple tries at training the same model, and more. For more information, check out this [blog post](https://wandb.ai/wandb_fc/repo-spotlight/reports/Community-Spotlight-OpenSoundscape--Vmlldzo0MDcwMTI4). \n",
    "\n",
    "To use `wandb` logging, you will need to create an account on the [Weights and Biases website](https://wandb.ai/). The first time you use `wandb`, you'll be asked for an authentication key which can be found in your `wandb` profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "113a1a3c-1b0b-4159-83d7-43f7cc1a0d24",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrhine3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "wandb: ERROR Error while calling W&B API: entity entity_name not found during upsertBucket (<Response [404]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_73952/377888475.py 3 <module>\n",
      "failed to create wandb session. wandb session will be None\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb_session = wandb.init(\n",
    "        entity='entity_name', #replace with your entity/group name\n",
    "        project='opensoundscape training demo',\n",
    "        name='Notebook 02: Train CNN',\n",
    "    )\n",
    "except: #if wandb.init fails, don't use wandb logging\n",
    "    print('failed to create wandb session. wandb session will be None')\n",
    "    wandb_session = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865c2ff-441b-40eb-a6d9-7665452c5add",
   "metadata": {},
   "source": [
    "## Train the CNN\n",
    "\n",
    "Finally, train the CNN for 30 epochs. Each **epoch** is one pass-through of all of the samples in the training dataset, plus running predictions on the validation dataset.\n",
    "\n",
    "The samples in the training dataset are predicted on in smaller groups of samples called **batches**. The machine learning model predicts on every sample in the batch, then the model weights are updated based on those samples. Larger batches can increase training speed, but require more memory. If you get a memory error, try reducing the batch size.\n",
    "\n",
    "We use default training parameters, but many aspects of CNN training can be customized (see the \"Customize CNN training\" tutorial for examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981bffa6-842e-4e76-bbf1-ad92a3a72dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = Path(\"model_training_checkpoints\")\n",
    "checkpoint_folder.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ea86e7f-5533-4815-bf34-31e141002dd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-32:\n",
      "Process Process-14:\n",
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/process.py\", line 316, in _bootstrap\n",
      "    exitcode = 0\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/multiprocessing/process.py\", line 316, in _bootstrap\n",
      "    exitcode = 0\n",
      "KeyboardInterrupt\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpjmfp5bftwandb'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpwm9jobpowandb-artifacts'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmp2z952hkpwandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpeitla1urwandb'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpw40zeojbwandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpswwvfn74wandb-artifacts'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpvpqd1ds7wandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpih3ix5s2wandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmptue_so8wwandb'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpkhpjsxgkwandb-artifacts'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmpyc04iyg2wandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "/Users/tessa/opt/anaconda3/envs/opso-dev/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/tmp1ch87vsjwandb-media'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbalanced_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#log progress every 100 batches\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#32 parallelized cpu tasks for preprocessing\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_session\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwandb_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#save checkpoint every 10 epochs\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#location to save checkpoints\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/opensoundscape/opensoundscape/ml/cnn.py:916\u001b[0m, in \u001b[0;36mCNN.train\u001b[0;34m(self, train_df, validation_df, epochs, batch_size, num_workers, save_path, save_interval, log_interval, validation_interval, invalid_samples_log, raise_errors, wandb_session, progress_bar)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;66;03m# 1 epoch = 1 view of each training file\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# loss fn & backpropogation occurs after each batch\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m### Training ###\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 916\u001b[0m     train_targets, train_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m### Evaluate ###\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     train_score, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\n\u001b[1;32m    922\u001b[0m         train_targets, train_scores\n\u001b[1;32m    923\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/opensoundscape/opensoundscape/ml/cnn.py:645\u001b[0m, in \u001b[0;36mCNN._train_epoch\u001b[0;34m(self, train_loader, wandb_session, progress_bar)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# backward pass: calculate the gradients\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# update the network using the gradients*lr\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_net\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model.train(\n",
    "    balanced_train_df, \n",
    "    valid_df, \n",
    "    epochs = 30, \n",
    "    batch_size = 64, \n",
    "    log_interval = 100, #log progress every 100 batches\n",
    "    num_workers = 32, #32 parallelized cpu tasks for preprocessing\n",
    "    wandb_session = wandb_session,\n",
    "    save_interval = 10, #save checkpoint every 10 epochs\n",
    "    save_path = checkpoint_folder #location to save checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b498f89-d856-45b5-bfe6-b9e94e603ada",
   "metadata": {},
   "source": [
    "**Clean up:** Run the following cell to delete the files created in this tutorial. However, these files are used in other tutorials, so you may wish not to delete them just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "440ca518-abcd-4bac-94e8-12ff8b8e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./annotated_data')\n",
    "shutil.rmtree('./wandb')\n",
    "shutil.rmtree('./model_training_checkpoints')\n",
    "Path('annotation_Files.zip').unlink()\n",
    "Path('mp3_Files.zip').unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402071e-ed20-4e76-96d1-a9afafc42475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso-dev",
   "language": "python",
   "name": "opso-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
