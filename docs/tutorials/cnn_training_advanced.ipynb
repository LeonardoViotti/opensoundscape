{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing CNN Training\n",
    "This notebook demonstrates how to use Opensoundscape.torch.cnn classes to\n",
    "\n",
    "- schedule the learning rate decay\n",
    "\n",
    "- choose from various architectures\n",
    "\n",
    "- use strategic sampling for imbalanced training data\n",
    "\n",
    "- train on spectrograms with a bandpassed frequency range\n",
    "\n",
    "Rather than demonstrating their effects on training (model training is slow!), most examples in this notebook either don't run .train() or \"train\" the model for 0 epochs for the purpose of demonstration\n",
    "\n",
    "For introductory demos (basic training, prediction, saving/loading models), see the basic training and prediction notebook tutorial (cnn.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess.preprocessors import BasePreprocessor, AudioToSpectrogramPreprocessor, CnnPreprocessor\n",
    "from opensoundscape.torch.models.cnn import PytorchModel, Resnet18Multiclass, Resnet18Binary, InceptionV3\n",
    "from opensoundscape.helpers import run_command\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for big visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "### Download labeled audio files\n",
    "\n",
    "The Kitzes Lab has created a small labeled dataset of short clips of American Woodcock vocalizations. You have two options for obtaining the folder of data, called `woodcock_labeled_data`:\n",
    "\n",
    "1. Run the following cell to download this small dataset. These commands require you to have `curl` and `tar` installed on your computer, as they will download and unzip a compressed file in `.tar.gz` format. \n",
    "\n",
    "2. Download a `.zip` version of the files by clicking [here](https://pitt.box.com/shared/static/m0cmzebkr5qc49q9egxnrwwp50wi8zu5.zip). You will have to unzip this folder and place the unzipped folder in the same folder that this notebook is in.\n",
    "\n",
    "If you already have these files, you can skip or comment out this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    \"curl -L https://pitt.box.com/shared/static/79fi7d715dulcldsy6uogz02rsn5uesd.gz -o ./woodcock_labeled_data.tar.gz\",\n",
    "    \"tar -xzf woodcock_labeled_data.tar.gz\", # Unzip the downloaded tar.gz file\n",
    "    \"rm woodcock_labeled_data.tar.gz\" # Remove the file after its contents are unzipped\n",
    "]\n",
    "for command in commands:\n",
    "    run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create one-hot labels\n",
    "see Basic Training/Prediction notebook for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/d4c40b6066b489518f8da83af1ee4984.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/79678c979ebb880d5ed6d56f26ba69ff.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/49890077267b569e142440fa39b3041c.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0c453a87185d8c7ce05c5c5ac5d525dc.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    negative  positive\n",
       "filename                                                              \n",
       "./woodcock_labeled_data/d4c40b6066b489518f8da83...         0         1\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...         1         0\n",
       "./woodcock_labeled_data/79678c979ebb880d5ed6d56...         0         1\n",
       "./woodcock_labeled_data/49890077267b569e142440f...         0         1\n",
       "./woodcock_labeled_data/0c453a87185d8c7ce05c5c5...         0         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(Path(\"woodcock_labeled_data/woodcock_labels.csv\"))\n",
    "labels.filename = ['./woodcock_labeled_data/'+f for f in labels.filename]\n",
    "\n",
    "labels['negative']=[0 if label=='present' else 1 for label in labels['woodcock']]\n",
    "labels['positive']=[1 if label=='present' else 0 for label in labels['woodcock']]\n",
    "classes=['negative','positive']\n",
    "labels = labels.set_index('filename')[classes]\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,valid_df = train_test_split(labels,test_size=0.2,random_state=0)\n",
    "# for multi-class need at least a few images for each batch\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess.preprocessors import CnnPreprocessor\n",
    "\n",
    "train_dataset = CnnPreprocessor(train_df, overlay_df=train_df)\n",
    "\n",
    "valid_dataset = CnnPreprocessor(valid_df,overlay_df=valid_df,return_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training parameters\n",
    "Let's take a peak at the current parameters, stored in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'feature': {'params': <generator object Module.parameters at 0x14699a850>,\n",
       "  'lr': 0.001,\n",
       "  'momentum': 0.9,\n",
       "  'weight_decay': 0.0005},\n",
       " 'classifier': {'params': <generator object Module.parameters at 0x14699aa50>,\n",
       "  'lr': 0.01,\n",
       "  'momentum': 0.9,\n",
       "  'weight_decay': 0.0005}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Resnet18Binary(classes)\n",
    "model.optimizer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rates\n",
    "The learning rate determines how much the model changes its weights every time it calculates the loss function. \n",
    "\n",
    "In Resnet18Multiclass and Resnet18Binary, we can modify the learning rates for the feature extration and classification blocks of the network separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, we can specify a relatively fast learning rate for features and slower one for classifiers (though this might not be a good idea in practice) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n"
     ]
    }
   ],
   "source": [
    "model = Resnet18Binary(classes)\n",
    "model.optimizer_params['feature']['lr']=0.01\n",
    "model.optimizer_params['classifier']['lr']=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay\n",
    "Weight decay performs L2 regularization - that is, it gives an incentive for the model to have small weights rather than large weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer_params['feature']['weight_decay']=0.001\n",
    "model.optimizer_params['classifier']['weight_decay']=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate schedule\n",
    "It's often helpful to decrease the learning rate over the course of training. By default, the learning rates are multiplied by 0.7 once every 10 epochs. \n",
    "\n",
    "Let's modify that for a very fast training schedule, where we want to multiply the learning rates by 0.1 every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lr_cooling_factor = 0.1\n",
    "model.lr_update_interval = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing network architecture\n",
    "We can swap out the model's CNN architecture as well, or design a custom one. \n",
    "\n",
    "A few architectures have custom classes built around them in the `cnn` module (Resnet and Inception, specifically). \n",
    "\n",
    "Here we show how to build a model from any of the architectures in `opensoundscape.torch.architectures.cnn_architectures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model with a Pytorch stock architecture\n",
    "Pytorch provides various well known CNN architectures out of the box. \n",
    "\n",
    "The `cnn_architectures` modules provides helper functions to generate various CNN architectures in Pytorch. Calling a function such as `alexnet()` will return a cnn architecture that we can use to instantiate PytorchModel. \n",
    "\n",
    "We can optionally \n",
    "\n",
    "- use pretrained weights provided by Pytorch (trained on Imagenet) - this often speeds up training significantly and is on by default\n",
    "\n",
    "- freeze the feature extractor if we only want to train the final classification layer of the network but not modify any other weights\n",
    "\n",
    "Here we'll set `use_pretrained=False` to avoid downloading all of the weights for AlexNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.torch.architectures.cnn_architectures import alexnet\n",
    "\n",
    "#initialize the AlexNet architecture\n",
    "arch = alexnet(num_classes=2,freeze_feature_extractor=False,use_pretrained=False)\n",
    "\n",
    "#generate a model object with this architecture\n",
    "model = PytorchModel(architecture=arch, classes=['negative','positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the architecture of an existing model\n",
    "Even after initializing a model with an architecture, we can change it by replacing the model's `.network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.architectures.cnn_architectures import densenet121\n",
    "\n",
    "#initialize the AlexNet architecture\n",
    "arch = densenet121(num_classes=2,freeze_feature_extractor=False,use_pretrained=False)\n",
    "\n",
    "# replace the alexnet architecture with the densenet architecture\n",
    "model.network = arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception V3 architecture\n",
    "The Inception architecture requires slightly different training and preprocessing, because\n",
    "\n",
    "1) the input image shape must be 299x299\n",
    "\n",
    "2) Inception's forward pass gives output + auxiliary output\n",
    "\n",
    "The InceptionV3 class in `cnn` handles the necessary modifications in training and prediction for you, but you'll need to make sure to pass images of the correct shape from your Preprocessor. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/Library/Caches/pypoetry/virtualenvs/opensoundscape-HOFcj3f8-py3.7/lib/python3.7/site-packages/torchvision/models/inception.py:82: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "Epoch: 0 [batch 0/6 (0.00%)] \n",
      "\tJacc: 0.500 Hamm: 0.500 DistLoss: 1.063\n",
      "\n",
      "Validation.\n",
      "(23, 2)\n",
      "\t Precision: 0.391304347826087\n",
      "\t Recall: 0.5\n",
      "\t F1: 0.4390243902439025\n",
      "Saving weights, metrics, and train/valid scores.\n",
      "Saving to epoch-0.model\n",
      "Updating best model\n",
      "Saving to best.model\n",
      "\n",
      "Best Model Appears at Epoch 0 with F1 0.439.\n",
      "(23, 2)\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.torch.models.cnn import InceptionV3\n",
    "\n",
    "#generate an Inception model\n",
    "model = InceptionV3(classes=['negative','positive'],use_pretrained=False)\n",
    "\n",
    "#create a copy of the training dataset\n",
    "inception_dataset = train_dataset.sample(frac=1)\n",
    "\n",
    "#modify the preprocessor to give 299x299 image shape\n",
    "inception_dataset.actions.to_img.set(shape=[299,299])\n",
    "\n",
    "#train and validate for 1 epoch\n",
    "#note that Inception will complain if batch_size=1\n",
    "model.train(inception_dataset,inception_dataset,epochs=1,batch_size=4)\n",
    "\n",
    "#predict\n",
    "preds, _, _ = model.predict(inception_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a custom-built architecture\n",
    "\n",
    "You can also build a custom architecture and initialize a PytorchModel model with it, or replace a model's `.network` with your custom architecture. \n",
    "\n",
    "For example, we can use the `architectures.resnet` module to build the ResNet50 architecture (just for demonstration - we could also simply create this architecture from the cnn_architectures module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "number of layers:\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# import a module that builds ResNet architecture from scratch\n",
    "from opensoundscape.torch.architectures.resnet import ResNetArchitecture\n",
    "\n",
    "#initialize the ResNet50 architecture\n",
    "net=ResNetArchitecture(\n",
    "            num_cls=2,\n",
    "            weights_init='ImageNet',\n",
    "            num_layers=50,\n",
    "        )\n",
    "\n",
    "#generate a regular resnet18 object\n",
    "model = Resnet18Multiclass(classes=['negative','positive'])\n",
    "\n",
    "#replace the model's network with the ResNet50 architecture\n",
    "model.network = net\n",
    "\n",
    "print('number of layers:')\n",
    "print(model.network.num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for imbalanced training data\n",
    "The imbalanced data sampler will help to ensure that a single batch contains only a few classes during training, and that the classes will recieve approximately equal representation within the batch. This is useful for _imbalanced_ training data (when some classes have far fewer training samples than others). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "\n",
      "Best Model Appears at Epoch 0 with F1 0.000.\n",
      "sampler:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<opensoundscape.torch.sampling.ImbalancedDatasetSampler at 0x14c4c0190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Resnet18Binary(classes)\n",
    "model.sampler = 'imbalanced' #default is None\n",
    "\n",
    "#...you can now train your model as normal\n",
    "model.train(train_dataset,valid_dataset,epochs=0)\n",
    "\n",
    "#once we run train(), we can see that the train_loader is using an ImbalancedDatasetSampler\n",
    "print('sampler:')\n",
    "model.train_loader.sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with custom preprocessors\n",
    "The preprocessing notebook tutorial gives in-depth descriptions of how to customize your preprocessing pipeline.\n",
    "\n",
    "Here, we'll just give a quick example of tweaking the preprocessing pipeline: providing the CNN with a bandpassed spectrogram object instead of the full frequency range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bandpassed spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "\n",
      "Best Model Appears at Epoch 0 with F1 0.000.\n"
     ]
    }
   ],
   "source": [
    "model = Resnet18Binary(classes)\n",
    "\n",
    "# turn onthe bandpass action of the datasets\n",
    "train_dataset.actions.bandpass.on()\n",
    "valid_dataset.actions.bandpass.on()\n",
    "\n",
    "#specify the min and max frequencies for the bandpass action\n",
    "train_dataset.actions.bandpass.set(min_f=3000,max_f=5000)\n",
    "valid_dataset.actions.bandpass.set(min_f=3000,max_f=5000)\n",
    "\n",
    "#now we can train and validate on the bandpassed spectrograms\n",
    "#don't forget that you'll need to apply the same bandpass actions to \n",
    "#any datasets that you use for predicting on new audio files \n",
    "model.train(train_dataset,valid_dataset,epochs=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean up\n",
    "remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for p in glob('./multilabel_train/*'):\n",
    "    Path(p).unlink()\n",
    "for p in glob('./woodcock_labeled_data/*'):\n",
    "    Path(p).unlink()\n",
    "for p in glob('./*.model'):\n",
    "    Path(p).unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso-develop",
   "language": "python",
   "name": "opensoundscape-hofcj3f8-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
