{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with pre-trained CNNs\n",
    "This notebook contains all the code you need to use a pre-trained OpenSoundscape convolutional neural network model (CNN) to make predictions on your own data. Before attempting this tutorial, install OpenSoundscape by following the instructions on the OpenSoundscape website, [opensoundscape.org](http://opensoundscape.org/). More detailed tutorials about data preprocessing, training CNNs, and customizing prediction methods can also be found on this site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages\n",
    "\n",
    "We will load several imports from OpenSoundscape. First, load the `AudiotoSpectrogramPreprocessor` class from the `preprocess.preprocessors` module. Preprocessor classes are used to load, transform, and augment audio samples for use in a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess.preprocessors import AudioToSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the `cnn` module provides classes for training and prediction with various structures of CNNs. For this example, load the `Resnet18Binary` class, used for models made with the Resnet18 architecture for predicting the presence or absence of a species (a \"binary\" classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape.torch.models.cnn import Resnet18Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load some additional packages and perform some setup for the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other utilities and packages\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare audio data for prediction\n",
    "\n",
    "To run predictions on your audio data, you will need to have your audio already split up into the clip lengths that the model expects to receive. If your audio data are not already split, see the demonstration of the `Audio.split()` method in the `audio_and_spectrogram` notebook.\n",
    "\n",
    "You can check the length of clips that the model to receives in the model's notes when you download it. This is often, but not always, 5.0 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download audio files\n",
    "\n",
    "The Kitzes Lab has created a small labeled dataset of short clips of American Woodcock vocalizations. You have two options for obtaining the folder of data, called `woodcock_labeled_data`:\n",
    "\n",
    "1. Run the following cell to download this small dataset. These commands require you to have `curl` and `tar` installed on your computer, as they will download and unzip a compressed file in `.tar.gz` format. \n",
    "\n",
    "2. OR download a `.zip` version of the files by clicking [here](https://pitt.box.com/shared/static/m0cmzebkr5qc49q9egxnrwwp50wi8zu5.zip). You will have to unzip this folder and place the unzipped folder in the same folder that this notebook is in.\n",
    "\n",
    "**Note**: Once you have the data, you do not need to run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['rm', 'woodcock_labeled_data.tar.gz'], returncode=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl','https://pitt.box.com/shared/static/79fi7d715dulcldsy6uogz02rsn5uesd.gz','-L', '-o','woodcock_labeled_data.tar.gz']) # Download the data\n",
    "subprocess.run([\"tar\",\"-xzf\", \"woodcock_labeled_data.tar.gz\"]) # Unzip the downloaded tar.gz file\n",
    "subprocess.run([\"rm\", \"woodcock_labeled_data.tar.gz\"]) # Remove the file after its contents are unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Preprocessor object\n",
    "\n",
    "In addition to having audio clips of the correct length, you will need to create a Preprocessor object that loads audio samples for the CNN. \n",
    "\n",
    "First, generate a Pandas DataFrame with the index containing the paths to each file, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a list of audio files\n",
    "file_list = glob('./woodcock_labeled_data/*.wav')\n",
    "\n",
    "# create a DataFrame with the audio files as the index\n",
    "audio_file_df = pd.DataFrame(index=file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use that DataFrame to create a Preprocessor object suitable for your application. Use the argument `return_labels=False`, as our audio to predict on does not have labels.\n",
    "\n",
    "If the model was trained with any special preprocesor settings, you should apply those settings here. For pretrained models created by the Kitzes Lab, see the model's notes from its download page for the exact code to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Preprocessor object \n",
    "# we use the option \"return_labels=False\" because our audio to predict on does not have labels\n",
    "from opensoundscape.preprocess.preprocessors import AudioToSpectrogramPreprocessor\n",
    "prediction_dataset = AudioToSpectrogramPreprocessor(audio_file_df, return_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models trained with OpenSoundscape v0.5.x\n",
    "Check the model notes page for the appropriate model class to use and import the correct class from the `cnn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import Resnet18Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration, let's generate a new Resnet18 model for binary prediction and save it to our local folder. This is a dummy model that will not be trained using any data and will thus not make meaningful predictions.\n",
    "\n",
    "If you download a pre-trained model, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "Saving to demo.model\n"
     ]
    }
   ],
   "source": [
    "model = Resnet18Binary(classes=['absent','present'])\n",
    "model.save('./demo.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import PytorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, provide the model class's `from_checkpoint()` method with the path to your downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "loading weights from saved object\n"
     ]
    }
   ],
   "source": [
    "# load the model into the appropriate model class\n",
    "model = Resnet18Binary.from_checkpoint('./demo.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions as follows. The `predict` method returns three arguments: scores, thresholded predictions, and labels. For unthresholded prediction on unlabeled data, only the first one is relevant, so we can discard the other returns using `scores, _, _`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2)\n"
     ]
    }
   ],
   "source": [
    "# call model.predict() with the Preprocessor to generate predictions\n",
    "scores, _, _ = model.predict(prediction_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the scores of the first 5 samples. These scores may be anything from negative to positive infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absent</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/6a83b011665c482c1f260d8e111aa81c.wav</th>\n",
       "      <td>0.484657</td>\n",
       "      <td>0.382294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0d043e9954d9d80ca2c3e86055e94487.wav</th>\n",
       "      <td>0.593123</td>\n",
       "      <td>0.313045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/78654b6f687d7635f50fba3546c7bdfa.wav</th>\n",
       "      <td>0.616126</td>\n",
       "      <td>0.423048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/863095c237c52ec51cff7395d70cee41.wav</th>\n",
       "      <td>0.287727</td>\n",
       "      <td>0.352490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>0.442259</td>\n",
       "      <td>0.363902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      absent   present\n",
       "./woodcock_labeled_data/6a83b011665c482c1f260d8...  0.484657  0.382294\n",
       "./woodcock_labeled_data/0d043e9954d9d80ca2c3e86...  0.593123  0.313045\n",
       "./woodcock_labeled_data/78654b6f687d7635f50fba3...  0.616126  0.423048\n",
       "./woodcock_labeled_data/863095c237c52ec51cff739...  0.287727  0.352490\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...  0.442259  0.363902"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the scores of the first 5 samples\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above returns the raw predictions of the model without any post-processing (such as a softmax layer or a sigmoid layer). \n",
    "\n",
    "For details on how to use the `predict()` function for post-processing of predictions and to generate binary 0/1 predictions of class presence, see the \"Basic training and prediction with CNNs\" tutorial notebook. But, as a quick example, let's add a softmax layer to make the prediction scores for both classes sum to 1. We can also use the `binary_preds` argument to generate 0/1 predictions for each sample and class. For presence/absence models, use the option `binary_preds='single_target'`. For multi-class models, think about whether each clip should be labeled with only one class (single target) or whether each clip could contain multiple classes (`binary_preds='multi_target'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2)\n"
     ]
    }
   ],
   "source": [
    "scores, binary_predictions, _ = model.predict(\n",
    "    prediction_dataset,\n",
    "    activation_layer='softmax',\n",
    "    binary_preds='single_target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the `scores` are continuous variables, but now have been softmaxed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absent</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/6a83b011665c482c1f260d8e111aa81c.wav</th>\n",
       "      <td>0.525568</td>\n",
       "      <td>0.474432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0d043e9954d9d80ca2c3e86055e94487.wav</th>\n",
       "      <td>0.569565</td>\n",
       "      <td>0.430435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/78654b6f687d7635f50fba3546c7bdfa.wav</th>\n",
       "      <td>0.548120</td>\n",
       "      <td>0.451880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/863095c237c52ec51cff7395d70cee41.wav</th>\n",
       "      <td>0.483815</td>\n",
       "      <td>0.516185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>0.519579</td>\n",
       "      <td>0.480421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      absent   present\n",
       "./woodcock_labeled_data/6a83b011665c482c1f260d8...  0.525568  0.474432\n",
       "./woodcock_labeled_data/0d043e9954d9d80ca2c3e86...  0.569565  0.430435\n",
       "./woodcock_labeled_data/78654b6f687d7635f50fba3...  0.548120  0.451880\n",
       "./woodcock_labeled_data/863095c237c52ec51cff739...  0.483815  0.516185\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...  0.519579  0.480421"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an additional output, the binary 0/1 (\"absent\" vs \"present\") predictions generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absent</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/6a83b011665c482c1f260d8e111aa81c.wav</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0d043e9954d9d80ca2c3e86055e94487.wav</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/78654b6f687d7635f50fba3546c7bdfa.wav</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/863095c237c52ec51cff7395d70cee41.wav</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    absent  present\n",
       "./woodcock_labeled_data/6a83b011665c482c1f260d8...     1.0      0.0\n",
       "./woodcock_labeled_data/0d043e9954d9d80ca2c3e86...     1.0      0.0\n",
       "./woodcock_labeled_data/78654b6f687d7635f50fba3...     1.0      0.0\n",
       "./woodcock_labeled_data/863095c237c52ec51cff739...     0.0      1.0\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...     1.0      0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to look at a histogram of the scores for the positive class. Because this dummy model had random weights, we would expect this histogram to center somewhere around 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 316,
       "width": 864
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(scores['present'],bins=20)\n",
    "_ = plt.xlabel('softmax score for positive class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on long (un-split) audio files\n",
    "\n",
    "It's also possible to run predictions on long audio files. In this case, OpenSoundscape will internally split the audio into short segments during prediction. The input and output of prediction is slightly different in this case:\n",
    "- Input is similar to before: a dataframe with the index containing the paths to audio files\n",
    "- Output is still a dataframe, but it will have three \"index\" columns. The first matches the index of the input, and contains the audio file paths. The second and third index columns contain the \"begin\" and \"end\" time of clips relative to the start of the audio file. The remaining columns, as usual, contain the names of each class and the scores or predictions for each class for that row's audio clip. \n",
    "\n",
    "Let's look at an example. We'll use the 1 minute audio file contained within OpenSoundscape's test folder as a \"long\" audio file. In practice, you can split files that are multiple hours long - the limiting factor is your computer's memory (\"RAM\"), which must be able to hold the entire audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensoundscape\n",
    "from opensoundscape.preprocess.preprocessors import LongAudioPreprocessor\n",
    "\n",
    "#get audio path from opensoundscape's tests folder\n",
    "audio_1m_path = Path(opensoundscape.__file__).parent.parent.joinpath('tests/audio/1min.wav')\n",
    "long_audio_prediction_df = pd.DataFrame(index=[audio_1m_path])\n",
    "img_shape = [224,224]\n",
    "\n",
    "#the audio will be split during prediction. choose the clip length and overlap of sequential clips (0 for no overlap)\n",
    "clip_length = 5.0\n",
    "clip_overlap = 0.0\n",
    "long_audio_prediction_ds = LongAudioPreprocessor(\n",
    "    long_audio_prediction_df,\n",
    "    audio_length=clip_length, \n",
    "    clip_overlap=clip_overlap, \n",
    "    out_shape=img_shape, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in addition to the scores (and potentially, predictions) the function returns a list of \"unsafe\" samples that caused errors during preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>absent</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">/home/louisfh/Development/opensoundscape/tests/audio/1min.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>0.784139</td>\n",
       "      <td>0.027767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>0.402279</td>\n",
       "      <td>-0.030964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>0.854899</td>\n",
       "      <td>0.172253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>0.626625</td>\n",
       "      <td>0.354499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>0.757906</td>\n",
       "      <td>0.259789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          absent  \\\n",
       "file                                               start_time end_time             \n",
       "/home/louisfh/Development/opensoundscape/tests/... 0.0        5.0       0.784139   \n",
       "                                                   5.0        10.0      0.402279   \n",
       "                                                   10.0       15.0      0.854899   \n",
       "                                                   15.0       20.0      0.626625   \n",
       "                                                   20.0       25.0      0.757906   \n",
       "\n",
       "                                                                         present  \n",
       "file                                               start_time end_time            \n",
       "/home/louisfh/Development/opensoundscape/tests/... 0.0        5.0       0.027767  \n",
       "                                                   5.0        10.0     -0.030964  \n",
       "                                                   10.0       15.0      0.172253  \n",
       "                                                   15.0       20.0      0.354499  \n",
       "                                                   20.0       25.0      0.259789  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df, pred_df, unsafe_samples = model.split_and_predict(\n",
    "    long_audio_prediction_ds,\n",
    "    file_batch_size=1,\n",
    "    num_workers=0,\n",
    "    activation_layer=None,\n",
    "    binary_preds='single_target',\n",
    "    threshold=0.5,\n",
    "    clip_batch_size=4,\n",
    "    error_log=None,\n",
    ")\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models trained with OpenSoundscape 0.4.x\n",
    "One set of our publicly availably [binary models for 500 species](https://pitt.app.box.com/s/3048856qbm9x55yi3zfksa3fide5uuf4) was created with an older version of OpenSoundscape. These models require a little bit of manipulation to load into OpenSoundscape 0.5.x and onward.\n",
    "\n",
    "First, let's download one of these models (it's stored in a .tar format) and save it to the same directory as this notebook in a file called `opso_04_model_acanthis-flammea.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['curl', 'https://pitt.box.com/shared/static/lglpty35omjhmq6cdz8cfudm43nn2t9f.tar', '-L', '-o', 'opso_04_model_acanthis-flammea.tar'], returncode=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl',\n",
    "               'https://pitt.box.com/shared/static/lglpty35omjhmq6cdz8cfudm43nn2t9f.tar', \n",
    "                '-L', '-o', 'opso_04_model_acanthis-flammea.tar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the weights from that model into an OpenSoundscape model object with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opensoundscape.torch.models.cnn import PytorchModel\n",
    "from opensoundscape.torch.architectures.cnn_architectures import resnet18\n",
    "import torch\n",
    "\n",
    "# load the tar file into a dictionary \n",
    "# (you could change this to the location of any .tar file on your computer)\n",
    "opso_04_model_tar_path = \"./opso_04_model_acanthis-flammea.tar\"\n",
    "opso_04_model_dict = torch.load(opso_04_model_tar_path)\n",
    "\n",
    "# create a resnet18 binary model \n",
    "# (all models created with Opensoundscape 0.4.x are 2-class resnet18 architectures)\n",
    "architecture = resnet18(num_classes=2,use_pretrained=False)\n",
    "model = PytorchModel(classes=['negative','positive'],architecture=architecture)\n",
    "\n",
    "# load the model weights into our model object\n",
    "# now, our model is equivalent to the trained model we downloaded\n",
    "model.network.load_state_dict(opso_04_model_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the model as normal to create predictions on audio. We'll use the same `prediction_dataset` from above (which does not contain any Common redpoll).\n",
    "\n",
    "Remember to choose the `activation_layer` you desire. In this example, we'll assume we just want to generate scores, not binary predictions. We'll apply a softmax layer, then the logit transform, to the scores using the `activation_layer=\"softmax_and_logit\"` option. This will generate the type of scores that are useful for plotting score histograms, among other things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/6a83b011665c482c1f260d8e111aa81c.wav</th>\n",
       "      <td>0.998181</td>\n",
       "      <td>0.001819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0d043e9954d9d80ca2c3e86055e94487.wav</th>\n",
       "      <td>0.999854</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/78654b6f687d7635f50fba3546c7bdfa.wav</th>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/863095c237c52ec51cff7395d70cee41.wav</th>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>0.995471</td>\n",
       "      <td>0.004529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    negative  positive\n",
       "./woodcock_labeled_data/6a83b011665c482c1f260d8...  0.998181  0.001819\n",
       "./woodcock_labeled_data/0d043e9954d9d80ca2c3e86...  0.999854  0.000145\n",
       "./woodcock_labeled_data/78654b6f687d7635f50fba3...  0.998935  0.001065\n",
       "./woodcock_labeled_data/863095c237c52ec51cff739...  0.999816  0.000184\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...  0.995471  0.004529"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions on our dataset\n",
    "predition_scores_df,_,_ = model.predict(prediction_dataset, activation_layer='softmax')\n",
    "predition_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the downloaded files to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path('./woodcock_labeled_data')\n",
    "[p.unlink() for p in folder.glob(\"*\")]\n",
    "folder.rmdir()\n",
    "for p in Path('.').glob('*.model'):\n",
    "    p.unlink()\n",
    "for p in Path('.').glob('*.tar'):\n",
    "    p.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpSoPoetry",
   "language": "python",
   "name": "opensoundscape-ayasfgeb-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
