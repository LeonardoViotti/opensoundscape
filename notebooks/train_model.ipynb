{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: a machine learning model in OpenSoundscape\n",
    "\n",
    "OpenSoundscape is a utility library for bioacoustic analyses. This quickstart will guide you through the process of creating a simple machine learning model that can identify the \"peent\" vocalization of an American Woodcock (*Scolopax minor*).\n",
    "\n",
    "To use this notebook, follow the \"developer\" installation instructions in OpenSoundscape's README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tessa/Library/Caches/pypoetry/virtualenvs/opensoundscape-dxMTH98s-py3.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/tessa/Library/Caches/pypoetry/virtualenvs/opensoundscape-dxMTH98s-py3.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.datasets import SingleTargetAudioDataset\n",
    "from opensoundscape.torch.train import train\n",
    "from opensoundscape.data_selection import binary_train_valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from math import floor\n",
    "from sklearn.utils import shuffle\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "from os.path import sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download labeled audio files\n",
    "\n",
    "The Kitzes Lab has created some labeled ARU data of American Woodcock vocalizations. Run the following cell to download this small dataset. \n",
    "\n",
    "These commands require you to have `wget` and `tar` installed on your computer, as they will download and unzip a compressed file in `.tar.gz` format. If you would prefer, you can also download a `.zip` version of the files by clicking [here](https://pitt.box.com/shared/static/m0cmzebkr5qc49q9egxnrwwp50wi8zu5.zip). You will have to unzip this folder and place it in the same folder that this notebook is in.\n",
    "\n",
    "The folder's name is `woodcock_labeled_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q --content-disposition https://pitt.box.com/shared/static/79fi7d715dulcldsy6uogz02rsn5uesd.gz;\n",
    "tar -xzf woodcock_labeled_data.tar.gz # Unzip the downloaded tar.gz file\n",
    "rm woodcock_labeled_data.tar.gz # Remove the file after its contents are unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder contains 2s long clips. It also contains a file `woodcock_labels.csv` which contains the names of each file and its corresponding label information, created using a program called [Specky](https://github.com/rhine3/specky).\n",
    "\n",
    "Create a pandas DataFrame of all of the labeled files, then inspect the `head()` of this dataframe to see what its contents look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>woodcock</th>\n",
       "      <th>sound_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d4c40b6066b489518f8da83af1ee4984.wav</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e84a4b60a4f2d049d73162ee99a7ead8.wav</td>\n",
       "      <td>absent</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79678c979ebb880d5ed6d56f26ba69ff.wav</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49890077267b569e142440fa39b3041c.wav</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0c453a87185d8c7ce05c5c5ac5d525dc.wav</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename woodcock sound_type\n",
       "0  d4c40b6066b489518f8da83af1ee4984.wav  present       song\n",
       "1  e84a4b60a4f2d049d73162ee99a7ead8.wav   absent         na\n",
       "2  79678c979ebb880d5ed6d56f26ba69ff.wav  present       song\n",
       "3  49890077267b569e142440fa39b3041c.wav  present       song\n",
       "4  0c453a87185d8c7ce05c5c5ac5d525dc.wav  present       song"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(Path(\"woodcock_labeled_data/woodcock_labels.csv\"))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the machine learning algorithm can find these files, add the name of the folder in front of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>woodcock</th>\n",
       "      <th>sound_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woodcock_labeled_data/d4c40b6066b489518f8da83a...</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woodcock_labeled_data/e84a4b60a4f2d049d73162ee...</td>\n",
       "      <td>absent</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woodcock_labeled_data/79678c979ebb880d5ed6d56f...</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woodcock_labeled_data/49890077267b569e142440fa...</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woodcock_labeled_data/0c453a87185d8c7ce05c5c5a...</td>\n",
       "      <td>present</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename woodcock sound_type\n",
       "0  woodcock_labeled_data/d4c40b6066b489518f8da83a...  present       song\n",
       "1  woodcock_labeled_data/e84a4b60a4f2d049d73162ee...   absent         na\n",
       "2  woodcock_labeled_data/79678c979ebb880d5ed6d56f...  present       song\n",
       "3  woodcock_labeled_data/49890077267b569e142440fa...  present       song\n",
       "4  woodcock_labeled_data/0c453a87185d8c7ce05c5c5a...  present       song"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['filename'] = 'woodcock_labeled_data' + sep + labels['filename'].astype(str)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation datasets\n",
    "To use machine learning on these files, separate them into a \"training\" dataset, which will be used to teach the machine learning algorithm, and a \"validation\" dataset, which will be used to evaluate the algorithm's performance each epoch.\n",
    "\n",
    "The \"present\" labels in the `woodcock` column of the dataframe will be turned into 1s. All other labels will be turned into 0s. This is required by Pytorch, which doesn't accept string labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = binary_train_valid_split(input_df = labels, label_column='woodcock', label=\"present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of labels so future users of the model will be able to interpret the 0/1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0:'absent', 1:'scolopax-minor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn these dataframes into \"Datasets\" using the `SingleTargetAudioDataset` class. We have to specify the names of the columns in the dataframes to use this class. Once they are set up in this class, they can be used by the training algorithm. Data augmentation could be applied in this step, but is not demonstrated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SingleTargetAudioDataset(\n",
    "    df=train_df, label_dict=label_dict, label_column='NumericLabels', filename_column='filename')\n",
    "valid_dataset = SingleTargetAudioDataset(\n",
    "    df=valid_df, label_dict=label_dict, label_column='NumericLabels', filename_column='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the machine learning model\n",
    "Next, we will set up the architecture of our model and train it. The model architecture we will use is a combination of a feature extractor and a classifier. \n",
    "\n",
    "The feature extractor is a `resnet18` convolutional neural network. We call it with `pretrained=True`, so that we use a version of the model that somebody has already trained on another image dataset called ImageNet. Although spectrograms aren't the same type of images as the photographs used in ImageNet, using the pretrained model will allow the model to more quickly adapt to identifying spectrograms.\n",
    "\n",
    "The classifier is a `Linear` classifier. We have to set the input and output size for this classifier. It takes in the outputs of the feature extractor, so `in_features = model.fc.in_features`. The model identifies one species, so it has to be able to output a \"present\" or \"absent\" classification. Thus, `out_features=2`. A multi-species model would use `out_features=number_of_species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up architecture for the type of model we will use\n",
    "model = resnet18(pretrained = True)\n",
    "model.fc = Linear(in_features = model.fc.in_features, out_features = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a directory in which to save results, and then run the model. We set up the following parameters:\n",
    "* `save_dir`: the directory in which to save results (which is created if it doesn't exist)\n",
    "* `model`: the model set up in the previous cell\n",
    "* `train_dataset`: the training dataset created using `SingleTargetAudioDataset`\n",
    "* `optimizer`: the optimizer to use for training the algorithm\n",
    "* `loss_fn`: the loss function used to assess the algorithm's performance during training\n",
    "* `epochs`: the number of times the model will run through the training data\n",
    "* `log_every`: how frequently to save performance data and save intermediate machine learning weights (`log_every=1` will save every epoch)\n",
    "\n",
    "This function allows you to control more parameters, but they are not demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  Training.\n",
      "  Validating.\n",
      "  Validation results:\n",
      "    train_loss: 0.6183590526607904\n",
      "    train_accuracy: 0.7272727272727273\n",
      "    train_precision: [0.09090909 0.63636364]\n",
      "    train_recall: [0.09090909 0.63636364]\n",
      "    train_f1: [0.09090909 0.63636364]\n",
      "    valid_accuracy: 0.7142857142857143\n",
      "    valid_precision: [0.         0.71428571]\n",
      "    valid_recall: [0.         0.71428571]\n",
      "    valid_f1: [0.         0.71428571]\n",
      "  Saved results to model_train_results/epoch-0.tar.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "save_path = Path('model_train_results')\n",
    "if not save_path.exists(): save_path.mkdir()\n",
    "train(\n",
    "    save_dir = save_path,\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    valid_dataset = valid_dataset,\n",
    "    optimizer = SGD(model.parameters(), lr=1e-3),\n",
    "    loss_fn = CrossEntropyLoss(),\n",
    "    epochs=1,\n",
    "    log_every=1,\n",
    "    print_logging=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSoundscape",
   "language": "python",
   "name": "opensoundscape-dxmth98s-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
